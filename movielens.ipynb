{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfI9GqZoSI29"
   },
   "source": [
    "The objective of this notebook (project) is to test multiple recommendation systems for the movielens dataset. Here are the different models that we have tried to implement\n",
    "\n",
    "1- **Matrix Factorization techniques with SGD learning (https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)**\n",
    "- Can't use SVD because the matrix is sparse.\n",
    "- We replicate this behaviour using deep learning.\n",
    "- Add bias terms & regularization\n",
    "\n",
    "2- Wide & Deep for recommender systems: We have tried to implement the following paper https://arxiv.org/pdf/1606.07792.pdf from Google, however, as the paper states, the wide part is built using binary cross-product features. Those features would need to manually be built and would explode the amount of neurons in the wide part.\n",
    "More information on feature crosses: https://datascience.stackexchange.com/questions/57435/how-is-the-cross-product-transformation-defined-for-binary-features?newreg=2093b549d07e43db92e28eccecb6a73b\n",
    "\n",
    "3- Neural Collaborative Filtering\n",
    "\n",
    "Dataset used: http://files.grouplens.org/datasets/movielens/ml-1m-README.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notes to properly run the notebook\n",
    "# At the time of developping this notebook, tensorboard was not fully integrated in skorch\n",
    "# so it has to be installed from the sources\n",
    "# git clone https://github.com/skorch-dev/skorch.gitts && cd skorch && python setup.py install\n",
    "\n",
    "# The ipywidgets package needs to be installed to see the progressbar checkpoint\n",
    "# It also needs to be activated like this: jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important notes for JSGL\n",
    "# If doing gridsearch, don't activate the function from dataloaders that spawn multiprocesses, memory will be hogged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQZgn3JNuFXM"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skorch import NeuralNet\n",
    "from skorch.helper import predefined_split, SliceDataset\n",
    "from skorch.callbacks import BatchScoring, Checkpoint, EarlyStopping, EpochScoring, LRScheduler, TensorBoard, ProgressBar\n",
    "\n",
    "# Install latest Tensorflow build\n",
    "#!pip install -q tf-nightly-2.0-preview\n",
    "import tensorflow as tf\n",
    "from tensorflow import summary\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MYaQB02n1Md_",
    "outputId": "e0fedd2a-edd8-4fa4-c812-18752d9e332b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device  cpu\n",
      "Using torch version  1.0.1\n"
     ]
    }
   ],
   "source": [
    "# Torch parameters\n",
    "identifier = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(identifier)\n",
    "device = 'cpu'\n",
    "print('Using device ', device)\n",
    "\n",
    "print('Using torch version ', torch.__version__)\n",
    "\n",
    "torch.set_printoptions(precision=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "ZvYWqFf2uG9j",
    "outputId": "0a6594ad-c7da-407a-ca5a-b34cbbefbc86"
   },
   "outputs": [],
   "source": [
    "#!wget http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "#!unzip -o ml-1m.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j24pjXi6tMs6"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIFR-8xl-kQ8"
   },
   "outputs": [],
   "source": [
    "class rsdataset(Dataset):\n",
    "    def __init__(self, usersfile, moviesfile, ratingsfile, nrows=None):\n",
    "        \n",
    "        # Read files\n",
    "        self.movies = pd.read_csv(moviesfile, sep='::', names=['MovieID', 'Title', 'Genres'], engine='python')\n",
    "        self.users = pd.read_csv(usersfile, sep='::', names=['UserID', 'Gender', 'Age', 'Occupation', 'Zipcode'], engine='python')\n",
    "        self.ratings = pd.read_csv(ratingsfile, sep='::', names=['UserID', 'MovieID', 'Rating', 'Timestamp'], engine='python', nrows=nrows)\n",
    "        \n",
    "        assert self.users['UserID'].nunique() >= self.ratings['UserID'].nunique(), 'UserID with unknown information'\n",
    "        assert self.movies['MovieID'].nunique() >= self.ratings['MovieID'].nunique(), 'Movies with unknown information'\n",
    "\n",
    "        self.users_emb_columns = []\n",
    "        self.users_ohe_columns = []\n",
    "        self.movies_emb_columns = []\n",
    "        self.movies_ohe_columns = []\n",
    "        self.interact_columns = []\n",
    "\n",
    "        self.nusers = self.ratings['UserID'].nunique()\n",
    "        self.nmovies = self.ratings['MovieID'].nunique()\n",
    "\n",
    "        self.y_range = (self.ratings['Rating'].min(), self.ratings['Rating'].max())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        What have we learned regarding tensors and GPU memory\n",
    "        -----------------------------------------------------\n",
    "        I used to transfer one big chunk of data from RAM to cpu/gpu ... It\n",
    "        was very long because it took time. The problem was that ohe are encoded in int64.\n",
    "\n",
    "        It was the same thing for cpu and gpu. I thought that gpu would be longer because it has\n",
    "        to transfer different memory bus probably? But it was the same thing with the CPU, very long\n",
    "\n",
    "        I tried pinned_memory=True in the dataloader, and it was the same thing.\n",
    "\n",
    "        I shipped everything in memory and in __getitem__ I sliced everything after.\n",
    "        I saved a few seconds (5 seconds)\n",
    "\n",
    "        One of the biggest increase was when all the dataset was transformed in 5 tensors.\n",
    "        I tried changing num_workers and the speed increased a lot. \n",
    "\n",
    "        \"\"\"\n",
    "        return (((self.users_emb[idx])),\n",
    "                ((self.users_ohe[idx])),\n",
    "                ((self.movies_emb[idx])),\n",
    "                ((self.movies_ohe[idx])),\n",
    "                ((self.interact[idx]))), (self.y[idx])\n",
    "\n",
    "    def to_tensor(self):\n",
    "        self.users_emb = torch.from_numpy(self.ratings[self.users_emb_columns].values)\n",
    "        self.users_ohe = torch.from_numpy(self.ratings[self.users_ohe_columns].values)\n",
    "        self.movies_emb = torch.from_numpy(self.ratings[self.movies_emb_columns].values)\n",
    "        self.movies_ohe = torch.from_numpy(self.ratings[self.movies_ohe_columns].values)\n",
    "        self.interact = torch.from_numpy(self.ratings[self.interact_columns].values)\n",
    "        self.y = torch.tensor(self.y.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uINDlkiMB_Fo"
   },
   "outputs": [],
   "source": [
    "train = rsdataset('ml-1m/users.dat', 'ml-1m/movies.dat', 'ml-1m/ratings.dat', nrows=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O8ivFYVSM3nr"
   },
   "source": [
    "### Preprocessing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_w1QQER37El"
   },
   "outputs": [],
   "source": [
    "train.ratings = train.ratings.merge(train.movies, left_on='MovieID', right_on='MovieID')\n",
    "train.movies = train.ratings[train.movies.columns]\n",
    "\n",
    "train.ratings = train.ratings.merge(train.users, left_on='UserID', right_on='UserID')\n",
    "train.users = train.ratings[train.users.columns]\n",
    "\n",
    "train.y = train.ratings['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlQhqnXOYcmp"
   },
   "outputs": [],
   "source": [
    "# Label Encode users\n",
    "#columns = ['UserID', 'Gender', 'Age', 'Occupation', 'Zipcode']\n",
    "columns = ['UserID', 'Gender', 'Age', 'Occupation']\n",
    "train.ratings[columns] = train.ratings[columns].apply(preprocessing.LabelEncoder().fit_transform)\n",
    "#train.ratings[columns] = train.ratings[columns].astype('object')\n",
    "train.users_emb_columns = train.users_emb_columns + columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZ3jSa_dRMLA"
   },
   "outputs": [],
   "source": [
    "# Label Encode movies\n",
    "columns = ['MovieID']\n",
    "train.ratings[columns] = train.ratings[columns].apply(preprocessing.LabelEncoder().fit_transform)\n",
    "#train.ratings[columns] = train.ratings[columns].astype('object')\n",
    "train.movies_emb_columns = train.movies_emb_columns + columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2XcLbiz9lql"
   },
   "outputs": [],
   "source": [
    "# One Hot Encode users\n",
    "#columns = ['Gender', 'Age', 'Occupation', 'Zipcode']\n",
    "columns = ['Gender', 'Age', 'Occupation']\n",
    "ohe = preprocessing.OneHotEncoder(categories='auto', sparse=False, dtype='int8')\n",
    "ohe.fit(train.ratings[columns])\n",
    "train.ratings = pd.concat([train.ratings, pd.DataFrame(data=ohe.transform(train.ratings[columns]), columns=ohe.get_feature_names(columns))], axis=1)\n",
    "train.users_ohe_columns = train.users_ohe_columns + columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfVq6sd5NqiO"
   },
   "outputs": [],
   "source": [
    "# One Hot Encode movies (non exclusive)\n",
    "genres = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', \n",
    "          'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "for genre in genres:\n",
    "    genre = genre.replace('-', '')\n",
    "    column = 'Genre_' + str(genre)\n",
    "    train.ratings[column] = train.ratings['Genres'].apply(lambda x: 1 if genre in x else 0)\n",
    "    train.movies_ohe_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTH7wNbZp_jA"
   },
   "outputs": [],
   "source": [
    "int_genres_gender = \"\"\n",
    "for genre in train.movies_ohe_columns:\n",
    "    int_genres_gender = int_genres_gender + '+' +genre + ':Gender'\n",
    "\n",
    "int_genres_age = \"\"\n",
    "for genre in train.movies_ohe_columns:\n",
    "    int_genres_age = int_genres_gender + '+' + genre + ':Age'\n",
    "    \n",
    "interact = patsy.dmatrix(\"0 + Gender:Age + Gender:Occupation + Age:Occupation\"+int_genres_gender+int_genres_age, data=train.ratings.astype('object'), return_type='dataframe').astype('int8')\n",
    "interact = interact.astype('uint8')\n",
    "train.ratings = pd.concat([train.ratings, interact], axis=1)\n",
    "train.interact_columns = interact.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRSGsdEmMlPE"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Title' 'Genres'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-54d2d89fffd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Drop unused columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmovies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Genres'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Genres'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Zipcode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4100\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4101\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4102\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4103\u001b[0m         )\n\u001b[0;32m   4104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3912\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3913\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3914\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3916\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3944\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3945\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3946\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3947\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\recsys\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5340\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not found in axis\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5341\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Title' 'Genres'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Drop unused columns\n",
    "train.movies.drop(['Title', 'Genres'], inplace=True, axis=1)\n",
    "train.ratings.drop(['Title', 'Genres', 'Zipcode'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPTkxLKQMWhI"
   },
   "outputs": [],
   "source": [
    "train.to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hm1qo9It3zsb"
   },
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNXPv9DAmHBB"
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "train_size = int(0.8 * len(train))\n",
    "test_size = len(train) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "dataloaders['valid'] = torch.utils.data.DataLoader(valid_dataset, batch_size=4096, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oa-KNh5qkUh8"
   },
   "source": [
    "### Define structure of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "daekeS2zju4B",
    "outputId": "066d0111-a05c-4d8d-96df-06294888e231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepnwide(\n",
      "  (emb_UserID): Embedding(6040, 60)\n",
      "  (emb_Gender): Embedding(2, 60)\n",
      "  (emb_Age): Embedding(7, 60)\n",
      "  (emb_Occupation): Embedding(21, 60)\n",
      "  (emb_MovieID): Embedding(3706, 60)\n",
      "  (h1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (h2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (h3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout1): Dropout(p=0.5)\n",
      "  (dropout2): Dropout(p=0.5)\n",
      "  (dropout3): Dropout(p=0.5)\n",
      "  (last_layer): Linear(in_features=542, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class deepnwide(nn.Module):\n",
    "    \"\"\"\n",
    "    Set of hyperparams: params = {\n",
    "        'lr': [0.001, 0.01],\n",
    "        'module__size_emb': [30, 60, 120],\n",
    "        'module__dropout': [0.2, 0.5]\n",
    "}\n",
    "    Best run: -0.9766627748807272 {'lr': 0.001, 'module__dropout': 0.2, 'module__size_emb': 30}\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, users_emb, movies_emb, users_ohe, movies_ohe, interact, nemb, size_emb, y_range, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.name = 'deepnwide'\n",
    "        self.y_range = y_range\n",
    "\n",
    "        # wide\n",
    "        # ohe part - We don't need to specify nothing here\n",
    "        \n",
    "        # deep\n",
    "        self.emb_UserID = nn.Embedding(len(torch.unique(users_emb[:, 0])), size_emb)\n",
    "        self.emb_UserID.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_Gender = nn.Embedding(len(torch.unique(users_emb[:, 1])), size_emb)\n",
    "        self.emb_Gender.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_Age = nn.Embedding(len(torch.unique(users_emb[:, 2])), size_emb)\n",
    "        self.emb_Age.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_Occupation = nn.Embedding(len(torch.unique(users_emb[:, 3])), size_emb)\n",
    "        self.emb_Occupation.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_MovieID = nn.Embedding(len(torch.unique(movies_emb[:, 0])), size_emb)\n",
    "        self.emb_MovieID.weight.data.uniform_(-.01, .01)\n",
    "\n",
    "        # hidden layers\n",
    "        self.h1 = nn.Linear(nemb * size_emb, 100)\n",
    "        self.h2 = nn.Linear(100, 100)\n",
    "        self.h3 = nn.Linear(100, 100)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "\n",
    "        # final dense layer \n",
    "        self.last_layer = nn.Linear((interact.shape[1]) + (movies_ohe.shape[1]) + (100), 1)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Classic Matrix Factorization with bias term.\n",
    "        MSE: 1.07\n",
    "        \"\"\"\n",
    "        # Assign data\n",
    "        user_emb = X[0]\n",
    "        user_ohe = X[1]\n",
    "        movie_emb = X[2]\n",
    "        movie_ohe = X[3]\n",
    "        interact = X[4]\n",
    "        \n",
    "        UserID = user_emb[:, 0]\n",
    "        Gender = user_emb[:, 1]\n",
    "        Age = user_emb[:, 2]\n",
    "        Occupation = user_emb[:, 3]\n",
    "        MovieID = movie_emb[:, 0]\n",
    "\n",
    "        UserID = self.emb_UserID(UserID)\n",
    "        Gender = self.emb_Gender(Gender)\n",
    "        Age = self.emb_Age(Age)\n",
    "        Occupation = self.emb_Occupation(Occupation)\n",
    "        MovieID = self.emb_MovieID(MovieID)\n",
    "\n",
    "        emb = torch.cat([UserID,\n",
    "                         Age,\n",
    "                         Gender,\n",
    "                         Occupation,\n",
    "                         MovieID],\n",
    "                         dim=1)\n",
    "        \n",
    "        emb = F.relu(self.dropout1(self.h1(emb)))\n",
    "        emb = F.relu(self.dropout2(self.h2(emb)))\n",
    "        emb = F.relu(self.dropout3(self.h3(emb)))\n",
    "\n",
    "        result = self.last_layer(torch.cat([interact.float(), movie_ohe.float(), emb.float()], dim=1))\n",
    "\n",
    "        return (torch.sigmoid(result) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]).squeeze()\n",
    "\n",
    "\n",
    "model = deepnwide(train.users_emb, train.movies_emb, train.users_ohe, train.movies_ohe, train.interact, 5, 60, train.y_range, 0.5)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "MftgRHZB3rFd",
    "outputId": "a2c9651e-2867-49ab-af8a-6479828a8fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twoembeds(\n",
      "  (emb_UserID): Embedding(6040, 15)\n",
      "  (emb_MovieID): Embedding(3706, 15)\n",
      "  (emb_UserID_b): Embedding(6040, 1)\n",
      "  (emb_MovieID_b): Embedding(3706, 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class twoembeds(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The best results that we were able to achieve on a randomly split test dataset of size 0.2:\n",
    "    - embsize: 30\n",
    "    - LR: 0.005 with LRstep of gamma 0.1 step size 7\n",
    "    - batch size of 4096\n",
    "\n",
    "    Results:\n",
    "    epoch:18\n",
    "    RMSE: 0.8578\n",
    "    train MSE: 0.5658\n",
    "    valid MSE: 0.7359\n",
    "    dur: 25.3224    \n",
    "    \"\"\"\n",
    "    def __init__(self, size_emb):\n",
    "        super().__init__()\n",
    "\n",
    "        # set name of model\n",
    "        self.name = 'twoembeds'\n",
    "\n",
    "        # User and movie embeddings\n",
    "        self.emb_UserID = nn.Embedding(train.nusers, size_emb)\n",
    "        self.emb_MovieID = nn.Embedding(train.nmovies, size_emb)\n",
    "        self.emb_UserID.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_MovieID.weight.data.uniform_(-.01, .01)\n",
    "        \n",
    "        # User and movie embeddings weights\n",
    "        self.emb_UserID_b = nn.Embedding(train.nusers, 1)\n",
    "        self.emb_MovieID_b = nn.Embedding(train.nmovies, 1)\n",
    "        self.emb_UserID_b.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_MovieID_b.weight.data.uniform_(-.01, .01)\n",
    " \n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Classic Matrix Factorization with bias term.\n",
    "        Best RMSE on validation dataset: ~1.08\n",
    "        \"\"\"\n",
    "        user_emb = X[0]\n",
    "        user_ohe = X[1]\n",
    "        movie_emb = X[2]\n",
    "        movie_ohe = X[3]\n",
    "        interact = X[4]\n",
    "\n",
    "        UserID = user_emb[:, 0]\n",
    "        MovieID = movie_emb[:, 0]\n",
    "\n",
    "        user_emb = self.emb_UserID(UserID)\n",
    "        movie_emb = self.emb_MovieID(MovieID)\n",
    "\n",
    "        mult = (user_emb * movie_emb).sum(1)\n",
    "\n",
    "        # add bias\n",
    "        multb = mult + self.emb_UserID_b(UserID).squeeze() + self.emb_MovieID_b(MovieID).squeeze()\n",
    "\n",
    "        multb = multb.float()\n",
    "\n",
    "        return multb\n",
    "\n",
    "\n",
    "model = twoembeds(15)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_rBEpoBPd4w5"
   },
   "source": [
    "### Skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9XsVtTYOqGq"
   },
   "outputs": [],
   "source": [
    "# Earlystopping callback\n",
    "earlystopping = EarlyStopping(monitor='valid_loss', patience=5, threshold=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ockpmqrz9KYv"
   },
   "outputs": [],
   "source": [
    "# RMSE callback\n",
    "def rmseloss(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_scorer = make_scorer(rmseloss)\n",
    "\n",
    "epoch_rmse = EpochScoring(rmse_scorer, name='rmse_score', lower_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p62gAk8FgaZS"
   },
   "outputs": [],
   "source": [
    "# Checkpoint callback\n",
    "checkpoint = Checkpoint(monitor='rmse_score_best', f_params='params.pt', f_optimizer='optimizer.pt', f_history='history.json', f_pickle='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "knAy1XQYjgze"
   },
   "outputs": [],
   "source": [
    "# Learning rate scheduler callback\n",
    "lr_scheduler = LRScheduler(policy=\"StepLR\", step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy29DspnXU-S"
   },
   "outputs": [],
   "source": [
    "# Progressbar callback\n",
    "progressbar = ProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u4yfIEH2rPZE",
    "outputId": "af7fad5c-f72c-4a7c-f302-0edf072bf0ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/jsleroux/anaconda3/envs/recsys/bin/tensorboard\", line 10, in <module>\n",
       "    sys.exit(run_main())\n",
       "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/tensorboard/main.py\", line 64, in run_main\n",
       "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
       "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n",
       "    _run_main(main, args)\n",
       "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n",
       "    sys.exit(main(argv))\n",
       "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/tensorboard/program.py\", line 220, in main\n",
       "    server = self._make_server()\n",
       "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/tensorboard/program.py\", line 301, in _make_server\n",
       "    self.assets_zip_provider)\n",
       "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/tensorboard/backend/application.py\", line 154, in standard_tensorboard_wsgi\n",
       "    flags, plugin_loaders, data_provider, assets_zip_provider, multiplexer)\n",
       "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/tensorboard/backend/application.py\", line 222, in TensorBoardWSGIApp\n",
       "    return TensorBoardWSGI(tbplugins, flags.path_prefix)\n",
       "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/tensorboard/backend/application.py\", line 274, in __init__\n",
       "    raise ValueError('Duplicate plugins for name %s' % plugin.plugin_name)\n",
       "ValueError: Duplicate plugins for name projector"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard\n",
    "!rm -rf runs/*\n",
    "writer = SummaryWriter()\n",
    "%tensorboard --logdir 'runs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgVTuwedriTI"
   },
   "outputs": [],
   "source": [
    "deepnwidenet = NeuralNet(\n",
    "    deepnwide,\n",
    "    module__users_emb=train.users_emb,\n",
    "    module__movies_emb=train.movies_emb,\n",
    "    module__users_ohe=train.users_ohe,\n",
    "    module__movies_ohe=train.movies_ohe,\n",
    "    module__interact=train.interact,\n",
    "    module__nemb=5,\n",
    "    module__size_emb=30,\n",
    "    module__y_range=train.y_range,\n",
    "    module__dropout=0.2,\n",
    "    max_epochs=30,\n",
    "    lr=0.001,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    device=device,\n",
    "    iterator_train__batch_size=1024,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__batch_size=4096,\n",
    "    train_split=predefined_split(valid_dataset),\n",
    "    callbacks=[\n",
    "               earlystopping,\n",
    "               epoch_rmse,\n",
    "               #checkpoint,\n",
    "               lr_scheduler,\n",
    "               #TensorBoard(writer),\n",
    "               #progressbar\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6qm5_snshUi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    rmse_score    train_loss    valid_loss       dur\n",
      "-------  ------------  ------------  ------------  --------\n",
      "      1        0.9029        0.8925        0.8152  109.6266\n",
      "      2        0.8949        0.8114        0.8009  112.3800\n",
      "      3        0.8859        0.7915        0.7848  118.3299\n",
      "      4        0.8789        0.7723        0.7724  119.1442\n",
      "      5        0.8771        0.7565        0.7694  121.6192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=deepnwide(\n",
       "    (emb_UserID): Embedding(6040, 30)\n",
       "    (emb_Gender): Embedding(2, 30)\n",
       "    (emb_Age): Embedding(7, 30)\n",
       "    (emb_Occupation): Embedding(21, 30)\n",
       "    (emb_MovieID): Embedding(3706, 30)\n",
       "    (h1): Linear(in_features=150, out_features=100, bias=True)\n",
       "    (h2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (h3): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (dropout1): Dropout(p=0.2)\n",
       "    (dropout2): Dropout(p=0.2)\n",
       "    (dropout3): Dropout(p=0.2)\n",
       "    (last_layer): Linear(in_features=542, out_features=1, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepnwidenet.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "s4Ecaxc4euD1",
    "outputId": "2cea73fa-450e-48aa-8b0f-77e47127abda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   1 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed: 23.4min\n",
      "[Parallel(n_jobs=8)]: Done   3 tasks      | elapsed: 29.1min\n",
      "[Parallel(n_jobs=8)]: Done   4 tasks      | elapsed: 29.5min\n",
      "[Parallel(n_jobs=8)]: Done   5 tasks      | elapsed: 34.9min\n",
      "[Parallel(n_jobs=8)]: Done   6 tasks      | elapsed: 40.3min\n",
      "[Parallel(n_jobs=8)]: Done   7 tasks      | elapsed: 41.0min\n",
      "[Parallel(n_jobs=8)]: Done   8 tasks      | elapsed: 47.0min\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed: 47.9min\n",
      "[Parallel(n_jobs=8)]: Done  10 tasks      | elapsed: 52.2min\n",
      "[Parallel(n_jobs=8)]: Done  11 tasks      | elapsed: 62.8min\n",
      "[Parallel(n_jobs=8)]: Done  12 tasks      | elapsed: 68.2min\n",
      "[Parallel(n_jobs=8)]: Done  13 tasks      | elapsed: 72.5min\n",
      "[Parallel(n_jobs=8)]: Done  14 tasks      | elapsed: 80.8min\n",
      "[Parallel(n_jobs=8)]: Done  15 tasks      | elapsed: 81.7min\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed: 84.2min\n",
      "[Parallel(n_jobs=8)]: Done  17 tasks      | elapsed: 103.2min\n",
      "[Parallel(n_jobs=8)]: Done  18 tasks      | elapsed: 103.4min\n",
      "[Parallel(n_jobs=8)]: Done  19 tasks      | elapsed: 104.7min\n",
      "[Parallel(n_jobs=8)]: Done  20 tasks      | elapsed: 106.9min\n",
      "[Parallel(n_jobs=8)]: Done  21 tasks      | elapsed: 107.4min\n",
      "[Parallel(n_jobs=8)]: Done  22 tasks      | elapsed: 118.3min\n",
      "[Parallel(n_jobs=8)]: Done  23 tasks      | elapsed: 122.7min\n",
      "[Parallel(n_jobs=8)]: Done  24 tasks      | elapsed: 123.6min\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed: 129.1min\n",
      "[Parallel(n_jobs=8)]: Done  26 tasks      | elapsed: 141.3min\n",
      "[Parallel(n_jobs=8)]: Done  27 tasks      | elapsed: 142.7min\n",
      "[Parallel(n_jobs=8)]: Done  28 tasks      | elapsed: 143.1min\n",
      "[Parallel(n_jobs=8)]: Done  29 tasks      | elapsed: 143.1min\n",
      "[Parallel(n_jobs=8)]: Done  30 out of  36 | elapsed: 144.9min remaining: 29.0min\n",
      "[Parallel(n_jobs=8)]: Done  31 out of  36 | elapsed: 150.4min remaining: 24.3min\n",
      "[Parallel(n_jobs=8)]: Done  32 out of  36 | elapsed: 151.8min remaining: 19.0min\n",
      "[Parallel(n_jobs=8)]: Done  33 out of  36 | elapsed: 153.2min remaining: 13.9min\n",
      "[Parallel(n_jobs=8)]: Done  34 out of  36 | elapsed: 173.0min remaining: 10.2min\n",
      "[Parallel(n_jobs=8)]: Done  36 out of  36 | elapsed: 181.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  36 out of  36 | elapsed: 181.9min finished\n",
      "-0.9766627748807272 {'lr': 0.001, 'module__dropout': 0.2, 'module__size_emb': 30}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'module__size_emb': [30, 60, 120],\n",
    "    'module__dropout': [0.2, 0.5]\n",
    "}\n",
    "gs = GridSearchCV(deepnwidenet,\n",
    "                  params,\n",
    "                  verbose=50,\n",
    "                  refit=False,\n",
    "                  pre_dispatch=8,\n",
    "                  n_jobs=8,\n",
    "                  cv=3,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "X_ds = SliceDataset(train, idx=0)\n",
    "y_ds = SliceDataset(train, idx=1)\n",
    "gs.fit(X_ds, y_ds)\n",
    "\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_9-m9v_3q03"
   },
   "outputs": [],
   "source": [
    "twoembedsnet = NeuralNet(\n",
    "    twoembeds,\n",
    "    module__size_emb=128,\n",
    "    max_epochs=30,\n",
    "    lr=0.001,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    device=device,\n",
    "    iterator_train__batch_size=4096,\n",
    "    iterator_train__num_workers=4,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__batch_size=4096,\n",
    "    train_split=predefined_split(valid_dataset),\n",
    "    callbacks=[earlystopping,\n",
    "               epoch_rmse,\n",
    "               #checkpoint,\n",
    "               lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9B_Z4LA6CPL"
   },
   "outputs": [],
   "source": [
    "twoembedsnet.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "G2QlpxyRTx7b",
    "outputId": "35483166-0d89-4148-c625-50c8884a9903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   52.1s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   54.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   55.9s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   57.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueManagerThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/externals/loky/process_executor.py\", line 662, in _queue_management_worker\n",
      "    for work_id, work_item in pending_work_items.items():\n",
      "RuntimeError: dictionary changed size during iteration\n",
      "\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGKILL(-9)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b583832601d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recsys/lib/python3.7/site-packages/scikit_learn-0.21.3-py3.7-linux-x86_64.egg/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recsys/lib/python3.7/site-packages/scikit_learn-0.21.3-py3.7-linux-x86_64.egg/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recsys/lib/python3.7/site-packages/scikit_learn-0.21.3-py3.7-linux-x86_64.egg/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recsys/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/recsys/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGKILL(-9)}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/externals/loky/backend/queues.py\", line 150, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/externals/loky/backend/reduction.py\", line 243, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/externals/loky/backend/reduction.py\", line 236, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/externals/cloudpickle/cloudpickle.py\", line 267, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 437, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 890, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 819, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 846, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 774, in save_tuple\n",
      "    save(element)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 774, in save_tuple\n",
      "    save(element)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 638, in save_reduce\n",
      "    save(args)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 774, in save_tuple\n",
      "    save(element)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 890, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 859, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 789, in save_tuple\n",
      "    save(element)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 819, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 843, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 510, in save\n",
      "    rv = reduce(obj)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/_memmapping_reducer.py\", line 339, in __call__\n",
      "    for dumped_filename in dump(a, filename):\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/numpy_pickle.py\", line 502, in dump\n",
      "    NumpyPickler(f, protocol=protocol).dump(value)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/pickle.py\", line 437, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/numpy_pickle.py\", line 289, in save\n",
      "    wrapper.write_array(obj, self)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/numpy_pickle.py\", line 104, in write_array\n",
      "    pickler.file_handle.write(chunk.tostring('C'))\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/externals/loky/backend/queues.py\", line 175, in _feed\n",
      "    onerror(e, obj)\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/externals/loky/process_executor.py\", line 310, in _on_queue_feeder_error\n",
      "    self.thread_wakeup.wakeup()\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/site-packages/joblib-0.13.2-py3.7.egg/joblib/externals/loky/process_executor.py\", line 155, in wakeup\n",
      "    self._writer.send_bytes(b\"\")\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/multiprocessing/connection.py\", line 183, in send_bytes\n",
      "    self._check_closed()\n",
      "  File \"/home/jsleroux/anaconda3/envs/recsys/lib/python3.7/multiprocessing/connection.py\", line 136, in _check_closed\n",
      "    raise OSError(\"handle is closed\")\n",
      "OSError: handle is closed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    #'lr': [0.001, 0.01],\n",
    "    'module__size_emb': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "}\n",
    "gs = GridSearchCV(twoembedsnet, params, verbose=50, refit=False, n_jobs=-1, cv=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "X_ds = SliceDataset(train, idx=0)\n",
    "y_ds = SliceDataset(train, idx=1)\n",
    "gs.fit(X_ds, y_ds)\n",
    "\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "c7FhrD3V6ZHC",
    "outputId": "7c3f91e6-fbf7-4c76-9dc2-508ba4e91325"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fef7a5fd9e8>>\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fef7a5fd9e8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fef7a5fd9e8>>\n",
      "    w.join()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "Traceback (most recent call last):\n",
      "AssertionError: can only join a child process\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    w.join()\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fef7a5fd9e8>>\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 883 ms, sys: 354 ms, total: 1.24 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "### Helper function to time the speed of the dataset in a dataloader\n",
    "for i in torch.utils.data.DataLoader(train_dataset, batch_size=4096, num_workers=4, shuffle=False):\n",
    "    #a, b = i\n",
    "    a = i\n",
    "    #for j in a:\n",
    "        #print(j.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFcj5YcEeuO3"
   },
   "outputs": [],
   "source": [
    "current_time = str(datetime.datetime.now().timestamp())\n",
    "train_log_dir = '/content/drive2/My Drive/tb/logs/tensorboard/train/' + current_time\n",
    "valid_log_dir = '/content/drive2/My Drive/tb/logs/tensorboard/valid/' + current_time\n",
    "train_summary_writer = summary.create_file_writer(train_log_dir)\n",
    "valid_summary_writer = summary.create_file_writer(valid_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6QaTiHtHtrI"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
    "    since = time.time()\n",
    "    \n",
    "    globaliter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        globaliter +=1\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "            \n",
    "            running_loss = 0.0\n",
    "\n",
    "            #for users_emb, users_ohe, movies_emb, movies_ohe, interact, labels in dataloaders[phase]:\n",
    "            for X, labels in dataloaders[phase]:\n",
    "   \n",
    "                #users_emb, users_ohe, movies_emb, movies_ohe, interact, labels = (Variable(users_emb.to(device)),\n",
    "                #                                                              Variable(users_ohe.to(device)), \n",
    "                #                                                                  Variable(movies_emb.to(device)),\n",
    "                #                                                                  Variable(movies_ohe.to(device)),\n",
    "                #                                                                  Variable(interact.to(device)),\n",
    "                #                                                                  Variable(labels.to(device)).float())\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(X)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()    \n",
    "                \n",
    "                #print(model.emb_UserID.weight.grad)\n",
    "\n",
    "                running_loss += loss.data\n",
    "            \n",
    "            epoch_loss = running_loss / (len(dataloaders[phase].dataset))\n",
    "            sqrt_loss = torch.sqrt(epoch_loss)\n",
    "            \n",
    "            # Tensorboard logging\n",
    "            if phase == 'train':\n",
    "                with train_summary_writer.as_default():\n",
    "                    tf.summary.scalar(model.name + ' RMSE', sqrt_loss.item(), step=globaliter)\n",
    "            else:\n",
    "                with valid_summary_writer.as_default():\n",
    "                    tf.summary.scalar(model.name + ' RMSE', sqrt_loss.item(), step=globaliter)\n",
    "            \n",
    "            print('{} loss: MSE: {:.6f} RMSE: {:.6f}'.format(phase, epoch_loss, sqrt_loss))\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "1nLS_9iytG-o",
    "outputId": "d5149ac5-67a9-45ec-abe6-5f30469dc317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-4cbe5d84b8a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#optimizer_ft = RAdam(model.parameters())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-158-b3e12c7ad84d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-149-039fcc6f805f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mMovieID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#print(MovieID.max(), train.movies)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0muser_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_UserID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUserID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mmovie_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_MovieID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMovieID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m         return F.embedding(\n\u001b[1;32m    116\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for argument #3 'index'"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss(reduction='sum') # we use sum because in the training loop, we divide by the length of the dataset\n",
    "#optimizer_ft = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-5)#, momentum=0.9, weight_decay=1e-5)\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "#optimizer_ft = RAdam(model.parameters())\n",
    "train_model(model, criterion, optimizer_ft, None, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3EYIwzIagqnC"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir '/content/drive2/My Drive/tb/logs/tensorboard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_WP8cIqZQWN"
   },
   "source": [
    "### Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pjhfBV6ZRUH"
   },
   "outputs": [],
   "source": [
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsJ69IyMZTcZ"
   },
   "outputs": [],
   "source": [
    "from surprise import NormalPredictor\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate, train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UG7J_vXTZjcf"
   },
   "outputs": [],
   "source": [
    "user = train[dataloaders['train'].dataset.indices][0][:, 0].data.numpy()\n",
    "movie = train[dataloaders['train'].dataset.indices][2][:, 0].data.numpy()\n",
    "y = train[dataloaders['train'].dataset.indices][5].data.numpy()\n",
    "df = pd.DataFrame({'user': user, 'movie': movie, 'y': y})\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['user', 'movie', 'y']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Khqq2oRQwyZ3"
   },
   "outputs": [],
   "source": [
    "data = Dataset.load_from_df(train.ratings.loc[dataloaders['train'].dataset.indices, ['UserID', 'MovieID', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZULF4SZexU81"
   },
   "outputs": [],
   "source": [
    "a = train.ratings.loc[dataloaders['train'].dataset.indices, ['UserID', 'MovieID', 'Rating']]\n",
    "b = pd.DataFrame({'UserID': user, 'MovieID': movie, 'Rating': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "osa_K_dkckBG"
   },
   "outputs": [],
   "source": [
    "#data = Dataset.load_builtin('ml-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePIcLN-Vawop"
   },
   "outputs": [],
   "source": [
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHY401zJgx8I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "movielens",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
