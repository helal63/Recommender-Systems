{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit feedback implementation of Recommender Systems based on MovieLens 1M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfI9GqZoSI29"
   },
   "source": [
    "The objective of this notebook (project) is to test multiple recommendation systems for the movielens dataset.  I wanted to learn more on the following subjects:\n",
    "- Pytorch: Learn more about this framework and implement some deep learning models\n",
    "- Skorch: Get rid of the infamous Pytorch training loop.\n",
    "- scikit-surprise: Learn how the framework works to obtain some benchmarks to compare my implementations\n",
    "\n",
    "The dataset that was used is MovieLens 1m: http://files.grouplens.org/datasets/movielens/ml-1m-README.txt . The main reason we used this dataset is because it's the biggest that contained side information.\n",
    "\n",
    "I tried to implement a simple version of the following papers:  \n",
    "1- **Matrix Factorization techniques with SGD learning** - https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf\n",
    "\n",
    "2- **Wide & Deep for recommender systems** - https://arxiv.org/pdf/1606.07792.pdf:\n",
    "The wide part of the model are manual interactions. Those interactions have been built using the patsy package.\n",
    "More information on feature crosses: https://datascience.stackexchange.com/questions/57435/how-is-the-cross-product-transformation-defined-for-binary-features\n",
    "\n",
    "3- **Neural Collaborative Filtering** - https://arxiv.org/pdf/1708.05031.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notes to properly run the notebook\n",
    "# At the time of developping this notebook, tensorboard was not fully integrated in skorch\n",
    "# so it has to be installed from the sources\n",
    "# git clone https://github.com/skorch-dev/skorch.git && cd skorch && python setup.py install\n",
    "\n",
    "# The ipywidgets package needs to be installed to see the progressbar checkpoint\n",
    "# It also needs to be activated like this: jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important notes for JSGL\n",
    "# If doing gridsearch, don't activate the function from dataloaders that spawn multiprocesses, memory will be hogged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQZgn3JNuFXM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import patsy\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skorch import NeuralNet\n",
    "from skorch.helper import predefined_split, SliceDataset\n",
    "from skorch.callbacks import BatchScoring, Checkpoint, EarlyStopping, EpochScoring, LRScheduler, TensorBoard, ProgressBar\n",
    "\n",
    "# Install latest Tensorflow build\n",
    "#!pip install -q tf-nightly-2.0-preview\n",
    "import tensorflow as tf\n",
    "from tensorflow import summary\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MYaQB02n1Md_",
    "outputId": "e0fedd2a-edd8-4fa4-c812-18752d9e332b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device  cpu\n",
      "Using torch version  1.2.0\n"
     ]
    }
   ],
   "source": [
    "# Torch parameters\n",
    "identifier = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(identifier)\n",
    "device = 'cpu'\n",
    "print('Using device ', device)\n",
    "\n",
    "print('Using torch version ', torch.__version__)\n",
    "\n",
    "torch.set_printoptions(precision=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "ZvYWqFf2uG9j",
    "outputId": "0a6594ad-c7da-407a-ca5a-b34cbbefbc86"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('ml-1m'):\n",
    "    !wget http://files.grouplens.org/datasets/movielens/ml-1m.zi\n",
    "    !unzip -o ml-1m.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j24pjXi6tMs6"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIFR-8xl-kQ8"
   },
   "outputs": [],
   "source": [
    "class rsdataset(Dataset):\n",
    "    def __init__(self, usersfile, moviesfile, ratingsfile, nrows=None):\n",
    "        \n",
    "        # Read files\n",
    "        self.movies = pd.read_csv(moviesfile, sep='::', names=['MovieID', 'Title', 'Genres'], engine='python')\n",
    "        self.users = pd.read_csv(usersfile, sep='::', names=['UserID', 'Gender', 'Age', 'Occupation', 'Zipcode'], engine='python')\n",
    "        self.ratings = pd.read_csv(ratingsfile, sep='::', names=['UserID', 'MovieID', 'Rating', 'Timestamp'], engine='python', nrows=nrows)\n",
    "        \n",
    "        assert self.users['UserID'].nunique() >= self.ratings['UserID'].nunique(), 'UserID with unknown information'\n",
    "        assert self.movies['MovieID'].nunique() >= self.ratings['MovieID'].nunique(), 'Movies with unknown information'\n",
    "\n",
    "        self.users_emb_columns = []\n",
    "        self.users_ohe_columns = []\n",
    "        self.movies_emb_columns = []\n",
    "        self.movies_ohe_columns = []\n",
    "        self.interact_columns = []\n",
    "\n",
    "        self.nusers = self.ratings['UserID'].nunique()\n",
    "        self.nmovies = self.ratings['MovieID'].nunique()\n",
    "\n",
    "        self.y_range = (self.ratings['Rating'].min(), self.ratings['Rating'].max())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        What have we learned regarding tensors and GPU memory\n",
    "        -----------------------------------------------------\n",
    "        - For every type of data, use the smalleset memory size required. For example, don't use int64 for ohe.\n",
    "        - pinned_memory=True didn't help my speed problems when I tried.\n",
    "        - As a consequence, everything was put in memory, and I __getitem__ was used to slice data.\n",
    "        - num_workers helped improving the speed. \n",
    "        \"\"\"\n",
    "        return (((self.users_emb[idx])),\n",
    "                ((self.users_ohe[idx])),\n",
    "                ((self.movies_emb[idx])),\n",
    "                ((self.movies_ohe[idx])),\n",
    "                ((self.interact[idx]))), (self.y[idx])\n",
    "\n",
    "    def to_tensor(self):\n",
    "        self.users_emb = torch.from_numpy(self.ratings[self.users_emb_columns].values)\n",
    "        self.users_ohe = torch.tensor(self.ratings[self.users_ohe_columns].values, dtype=torch.float)\n",
    "        self.movies_emb = torch.from_numpy(self.ratings[self.movies_emb_columns].values)\n",
    "        self.movies_ohe = torch.tensor(self.ratings[self.movies_ohe_columns].values, dtype=torch.float)\n",
    "        self.interact = torch.from_numpy(self.ratings[self.interact_columns].values)\n",
    "        self.y = torch.tensor(self.y.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uINDlkiMB_Fo"
   },
   "outputs": [],
   "source": [
    "train = rsdataset('ml-1m/users.dat', 'ml-1m/movies.dat', 'ml-1m/ratings.dat', nrows=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O8ivFYVSM3nr"
   },
   "source": [
    "### Preprocessing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_w1QQER37El"
   },
   "outputs": [],
   "source": [
    "train.ratings = train.ratings.merge(train.movies, left_on='MovieID', right_on='MovieID')\n",
    "train.movies = train.ratings[train.movies.columns]\n",
    "\n",
    "train.ratings = train.ratings.merge(train.users, left_on='UserID', right_on='UserID')\n",
    "train.users = train.ratings[train.users.columns]\n",
    "\n",
    "train.y = train.ratings['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlQhqnXOYcmp"
   },
   "outputs": [],
   "source": [
    "# Label Encode users\n",
    "columns = ['UserID', 'Gender', 'Age', 'Occupation']\n",
    "train.ratings[columns] = train.ratings[columns].apply(preprocessing.LabelEncoder().fit_transform)\n",
    "train.users_emb_columns = train.users_emb_columns + columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZ3jSa_dRMLA"
   },
   "outputs": [],
   "source": [
    "# Label Encode movies\n",
    "columns = ['MovieID']\n",
    "train.ratings[columns] = train.ratings[columns].apply(preprocessing.LabelEncoder().fit_transform)\n",
    "train.movies_emb_columns = train.movies_emb_columns + columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2XcLbiz9lql"
   },
   "outputs": [],
   "source": [
    "# One Hot Encode users\n",
    "#columns = ['Gender', 'Age', 'Occupation', 'Zipcode']\n",
    "columns = ['Gender', 'Age', 'Occupation']\n",
    "ohe = preprocessing.OneHotEncoder(categories='auto', sparse=False, dtype='uint8')\n",
    "ohe.fit(train.ratings[columns])\n",
    "train.ratings = pd.concat([train.ratings, pd.DataFrame(data=ohe.transform(train.ratings[columns]), columns=ohe.get_feature_names(columns))], axis=1)\n",
    "train.users_ohe_columns = ohe.get_feature_names(columns)\n",
    "\n",
    "assert train.ratings[train.users_ohe_columns].max().max()<=1, 'Error with ohe columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfVq6sd5NqiO"
   },
   "outputs": [],
   "source": [
    "# One Hot Encode movies (non exclusive)\n",
    "genres = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', \n",
    "          'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "for genre in genres:\n",
    "    genre = genre.replace('-', '')\n",
    "    column = 'Genre_' + str(genre)\n",
    "    train.ratings[column] = train.ratings['Genres'].apply(lambda x: 1 if genre in x else 0)\n",
    "    train.movies_ohe_columns.append(column)\n",
    "    \n",
    "assert train.ratings[train.movies_ohe_columns].max().max()<=1, 'Error with ohe columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTH7wNbZp_jA"
   },
   "outputs": [],
   "source": [
    "int_genres_gender = \"\"\n",
    "for genre in train.movies_ohe_columns:\n",
    "    int_genres_gender = int_genres_gender + '+' +genre + ':Gender'\n",
    "\n",
    "int_genres_age = \"\"\n",
    "for genre in train.movies_ohe_columns:\n",
    "    int_genres_age = int_genres_age + '+' + genre + ':Age'\n",
    "    \n",
    "interact = patsy.dmatrix(\"0 + Gender:Age + Gender:Occupation + Age:Occupation\"+int_genres_gender+int_genres_age, data=train.ratings.astype('object'), return_type='dataframe').astype('int8')\n",
    "interact = interact.astype('uint8')\n",
    "train.ratings = pd.concat([train.ratings, interact], axis=1)\n",
    "train.interact_columns = interact.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRSGsdEmMlPE"
   },
   "outputs": [],
   "source": [
    "# Drop unused columns\n",
    "train.movies.drop(['Title', 'Genres'], inplace=True, axis=1)\n",
    "train.ratings.drop(['Title', 'Genres', 'Zipcode'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPTkxLKQMWhI"
   },
   "outputs": [],
   "source": [
    "train.to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hm1qo9It3zsb"
   },
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNXPv9DAmHBB"
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "train_size = int(0.8 * len(train))\n",
    "test_size = len(train) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "dataloaders['valid'] = torch.utils.data.DataLoader(valid_dataset, batch_size=4096, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oa-KNh5qkUh8"
   },
   "source": [
    "### Define Pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "daekeS2zju4B",
    "outputId": "066d0111-a05c-4d8d-96df-06294888e231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepnwide(\n",
      "  (emb_UserID): Embedding(70, 60)\n",
      "  (emb_Gender): Embedding(2, 60)\n",
      "  (emb_Age): Embedding(7, 60)\n",
      "  (emb_Occupation): Embedding(19, 60)\n",
      "  (emb_MovieID): Embedding(2159, 60)\n",
      "  (h1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (h2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (h3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (last_layer): Linear(in_features=404, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class deepnwide(nn.Module):\n",
    "    \"\"\"\n",
    "    Hyperparameters:\n",
    "        - module__size_emb\n",
    "        - module__dropout\n",
    "        - module__linear_size\n",
    "}\n",
    "    Best run: -0.9766627748807272 {'lr': 0.001, 'module__dropout': 0.2, 'module__size_emb': 30}\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, users_emb, movies_emb, users_ohe, movies_ohe, interact, size_emb, y_range, dropout, linear_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.name = 'deepnwide'\n",
    "        self.y_range = y_range\n",
    "\n",
    "        # wide part - We don't need to specify nothing here\n",
    "        \n",
    "        # deep\n",
    "        self.emb_UserID = nn.Embedding(len(torch.unique(users_emb[:, 0])), size_emb)\n",
    "        self.emb_UserID.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_Gender = nn.Embedding(len(torch.unique(users_emb[:, 1])), size_emb)\n",
    "        self.emb_Gender.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_Age = nn.Embedding(len(torch.unique(users_emb[:, 2])), size_emb)\n",
    "        self.emb_Age.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_Occupation = nn.Embedding(len(torch.unique(users_emb[:, 3])), size_emb)\n",
    "        self.emb_Occupation.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_MovieID = nn.Embedding(len(torch.unique(movies_emb[:, 0])), size_emb)\n",
    "        self.emb_MovieID.weight.data.uniform_(-.01, .01)\n",
    "\n",
    "        # hidden layers\n",
    "        self.h1 = nn.Linear(5 * size_emb, linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, linear_size)\n",
    "        self.h3 = nn.Linear(linear_size, linear_size)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "\n",
    "        # final dense layer \n",
    "        self.last_layer = nn.Linear((interact.shape[1]) + (movies_ohe.shape[1]) + (linear_size), 1)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Assign data\n",
    "        user_emb = X[0]\n",
    "        user_ohe = X[1]\n",
    "        movie_emb = X[2]\n",
    "        movie_ohe = X[3]\n",
    "        interact = X[4]\n",
    "        \n",
    "        UserID = user_emb[:, 0]\n",
    "        Gender = user_emb[:, 1]\n",
    "        Age = user_emb[:, 2]\n",
    "        Occupation = user_emb[:, 3]\n",
    "        MovieID = movie_emb[:, 0]\n",
    "\n",
    "        UserID = self.emb_UserID(UserID)\n",
    "        Gender = self.emb_Gender(Gender)\n",
    "        Age = self.emb_Age(Age)\n",
    "        Occupation = self.emb_Occupation(Occupation)\n",
    "        MovieID = self.emb_MovieID(MovieID)\n",
    "\n",
    "        emb = torch.cat([UserID,\n",
    "                         Age,\n",
    "                         Gender,\n",
    "                         Occupation,\n",
    "                         MovieID],\n",
    "                         dim=1)\n",
    "        \n",
    "        emb = F.relu(self.dropout1(self.h1(emb)))\n",
    "        emb = F.relu(self.dropout2(self.h2(emb)))\n",
    "        emb = F.relu(self.dropout3(self.h3(emb)))\n",
    "\n",
    "        result = self.last_layer(torch.cat([interact.float(), movie_ohe.float(), emb.float()], dim=1))\n",
    "\n",
    "        return (torch.sigmoid(result) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]).squeeze()\n",
    "\n",
    "\n",
    "model = deepnwide(train.users_emb, train.movies_emb, train.users_ohe, train.movies_ohe, train.interact, 60, train.y_range, 0.5, 100)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "MftgRHZB3rFd",
    "outputId": "a2c9651e-2867-49ab-af8a-6479828a8fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twoembeds(\n",
      "  (emb_UserID): Embedding(70, 15)\n",
      "  (emb_MovieID): Embedding(2159, 15)\n",
      "  (emb_UserID_b): Embedding(70, 1)\n",
      "  (emb_MovieID_b): Embedding(2159, 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class twoembeds(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Hyperparameters:\n",
    "        module__size_emb\n",
    "    \"\"\"\n",
    "    def __init__(self, size_emb, y_range):\n",
    "        super().__init__()\n",
    "\n",
    "        # set name of model\n",
    "        self.name = 'twoembeds'\n",
    "        self.y_range = y_range\n",
    "\n",
    "        # User and movie embeddings\n",
    "        self.emb_UserID = nn.Embedding(train.nusers, size_emb)\n",
    "        self.emb_MovieID = nn.Embedding(train.nmovies, size_emb)\n",
    "        self.emb_UserID.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_MovieID.weight.data.uniform_(-.01, .01)\n",
    "        \n",
    "        # User and movie embeddings weights\n",
    "        self.emb_UserID_b = nn.Embedding(train.nusers, 1)\n",
    "        self.emb_MovieID_b = nn.Embedding(train.nmovies, 1)\n",
    "        self.emb_UserID_b.weight.data.uniform_(-.01, .01)\n",
    "        self.emb_MovieID_b.weight.data.uniform_(-.01, .01)\n",
    " \n",
    "\n",
    "    def forward(self, X):\n",
    "        user_emb = X[0]\n",
    "        user_ohe = X[1]\n",
    "        movie_emb = X[2]\n",
    "        movie_ohe = X[3]\n",
    "        interact = X[4]\n",
    "\n",
    "        UserID = user_emb[:, 0]\n",
    "        MovieID = movie_emb[:, 0]\n",
    "\n",
    "        user_emb = self.emb_UserID(UserID)\n",
    "        movie_emb = self.emb_MovieID(MovieID)\n",
    "\n",
    "        mult = (user_emb * movie_emb).sum(1)\n",
    "\n",
    "        # add bias\n",
    "        multb = mult + self.emb_UserID_b(UserID).squeeze() + self.emb_MovieID_b(MovieID).squeeze()\n",
    "\n",
    "        multb = multb.float()\n",
    "\n",
    "        return (torch.sigmoid(multb) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]).squeeze()\n",
    "\n",
    "        return multb\n",
    "\n",
    "\n",
    "model = twoembeds(15, train.y_range)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncf(\n",
      "  (gmf_embuserid): Embedding(70, 60)\n",
      "  (gmf_embgender): Embedding(2, 60)\n",
      "  (gmf_embage): Embedding(7, 60)\n",
      "  (gmf_embocc): Embedding(19, 60)\n",
      "  (gmf_embmovieid): Embedding(2159, 222)\n",
      "  (mlp_embuserid): Embedding(70, 60)\n",
      "  (mlp_embgender): Embedding(2, 60)\n",
      "  (mlp_embage): Embedding(7, 60)\n",
      "  (mlp_embocc): Embedding(19, 60)\n",
      "  (mlp_embmovieid): Embedding(2159, 60)\n",
      "  (h1): Linear(in_features=318, out_features=200, bias=True)\n",
      "  (h2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (h3): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (last_layer): Linear(in_features=440, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ncf(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Collaborative Filtering: https://arxiv.org/pdf/1708.05031.pdf\n",
    "    There is a matrix factorization part in this model and a deep learning one.\n",
    "    \n",
    "    Hyper parameters:\n",
    "    - module__size_emb\n",
    "    - module__dropout\n",
    "    - module__linear_size\n",
    "    \"\"\"\n",
    "    def __init__(self, users_emb, movies_emb, users_ohe, movies_ohe, interact, size_emb, dropout, linear_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # set name of model\n",
    "        self.name = 'ncf'\n",
    "        \n",
    "        ### GMF part\n",
    "        # user embeddings\n",
    "        self.gmf_embuserid = nn.Embedding(len(torch.unique(users_emb[:, 0])), size_emb)\n",
    "        self.gmf_embuserid.weight.data.uniform_(-.01, .01)\n",
    "        self.gmf_embgender = nn.Embedding(len(torch.unique(users_emb[:, 1])), size_emb)\n",
    "        self.gmf_embgender.weight.data.uniform_(-.01, .01)\n",
    "        self.gmf_embage = nn.Embedding(len(torch.unique(users_emb[:, 2])), size_emb)\n",
    "        self.gmf_embage.weight.data.uniform_(-.01, .01)\n",
    "        self.gmf_embocc = nn.Embedding(len(torch.unique(users_emb[:, 3])), size_emb)\n",
    "        self.gmf_embocc.weight.data.uniform_(-.01, .01)\n",
    "        # movie embeddings\n",
    "        self.gmf_embmovieid = nn.Embedding(len(torch.unique(movies_emb[:, 0])), size_emb*4-len(train.movies_ohe_columns))\n",
    "        self.gmf_embmovieid.weight.data.uniform_(-.01, .01)\n",
    "        \n",
    "        \n",
    "        ### MLP part\n",
    "        # user embeddings\n",
    "        self.mlp_embuserid = nn.Embedding(len(torch.unique(users_emb[:, 0])), size_emb)\n",
    "        self.mlp_embuserid.weight.data.uniform_(-.01, .01)\n",
    "        self.mlp_embgender = nn.Embedding(len(torch.unique(users_emb[:, 1])), size_emb)\n",
    "        self.mlp_embgender.weight.data.uniform_(-.01, .01)\n",
    "        self.mlp_embage = nn.Embedding(len(torch.unique(users_emb[:, 2])), size_emb)\n",
    "        self.mlp_embage.weight.data.uniform_(-.01, .01)\n",
    "        self.mlp_embocc = nn.Embedding(len(torch.unique(users_emb[:, 3])), size_emb)\n",
    "        self.mlp_embocc.weight.data.uniform_(-.01, .01)\n",
    "        # movie embeddings\n",
    "        self.mlp_embmovieid = nn.Embedding(len(torch.unique(movies_emb[:, 0])), size_emb)\n",
    "        self.mlp_embmovieid.weight.data.uniform_(-.01, .01)\n",
    "        # hidden layers\n",
    "        self.h1 = nn.Linear(5*size_emb+len(train.movies_ohe_columns), linear_size)\n",
    "        self.h2 = nn.Linear(linear_size, linear_size)\n",
    "        self.h3 = nn.Linear(linear_size, linear_size)\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # final dense layer \n",
    "        self.last_layer = nn.Linear(size_emb*4+linear_size, 1)\n",
    "                                       \n",
    "    def forward(self, X):\n",
    "        user_emb = X[0]\n",
    "        user_ohe = X[1]\n",
    "        movie_emb = X[2]\n",
    "        movie_ohe = X[3]\n",
    "        interact = X[4]\n",
    "        \n",
    "        UserID = user_emb[:, 0]\n",
    "        Gender = user_emb[:, 1]\n",
    "        Age = user_emb[:, 2]\n",
    "        Occupation = user_emb[:, 3]\n",
    "        MovieID = movie_emb[:, 0]\n",
    "\n",
    "        # GMF part\n",
    "        gmf_embuserid = self.gmf_embuserid(UserID)\n",
    "        gmf_embgender = self.gmf_embgender(Gender)\n",
    "        gmf_embage = self.gmf_embage(Age)\n",
    "        gmf_embocc = self.gmf_embocc(Occupation)\n",
    "        gmf_embmovieid = self.gmf_embmovieid(MovieID)\n",
    "\n",
    "        gmf_user_vector = torch.cat([gmf_embuserid,\n",
    "                                    gmf_embgender,\n",
    "                                    gmf_embage,\n",
    "                                    gmf_embocc],\n",
    "                                    dim=1)\n",
    "\n",
    "        gmf_movie_vector = torch.cat([gmf_embmovieid, movie_ohe], 1)\n",
    "        \n",
    "        gmf_vector = (gmf_user_vector * gmf_movie_vector)\n",
    "\n",
    "        \n",
    "        # MLP part\n",
    "        mlp_embuserid = self.mlp_embuserid(UserID)\n",
    "        mlp_embgender = self.mlp_embgender(Gender)\n",
    "        mlp_embage = self.mlp_embage(Age)\n",
    "        mlp_embocc = self.mlp_embocc(Occupation)\n",
    "        mlp_movieid = self.mlp_embmovieid(MovieID)\n",
    "        \n",
    "        mlp_vector = torch.cat([mlp_embuserid,\n",
    "                                mlp_embgender,\n",
    "                                mlp_embage,\n",
    "                                mlp_embocc,\n",
    "                                mlp_movieid,\n",
    "                                movie_ohe],\n",
    "                                dim=1)\n",
    "        mlp_vector = F.relu(self.dropout1(self.h1(mlp_vector)))\n",
    "        mlp_vector = F.relu(self.dropout2(self.h2(mlp_vector)))\n",
    "        mlp_vector = F.relu(self.dropout3(self.h3(mlp_vector)))\n",
    "\n",
    "        # Fusion\n",
    "        result = torch.cat([gmf_vector, mlp_vector], dim=1)\n",
    "        result = self.last_layer(result)\n",
    "        \n",
    "        return (torch.sigmoid(result) * (5-1) + 1).squeeze()\n",
    "\n",
    "model = ncf(train.users_emb, train.movies_emb, train.users_ohe, train.movies_ohe, train.interact, 60, 0.5, 200)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_rBEpoBPd4w5"
   },
   "source": [
    "### Skorch callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9XsVtTYOqGq"
   },
   "outputs": [],
   "source": [
    "# Earlystopping callback\n",
    "earlystopping = EarlyStopping(monitor='valid_loss', patience=5, threshold=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ockpmqrz9KYv"
   },
   "outputs": [],
   "source": [
    "# RMSE callback\n",
    "def rmseloss(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_scorer = make_scorer(rmseloss)\n",
    "\n",
    "epoch_rmse = EpochScoring(rmse_scorer, name='rmse_score', lower_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p62gAk8FgaZS"
   },
   "outputs": [],
   "source": [
    "# Checkpoint callback\n",
    "checkpoint = Checkpoint(monitor='rmse_score_best', f_params='params.pt', f_optimizer='optimizer.pt', f_history='history.json', f_pickle='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "knAy1XQYjgze"
   },
   "outputs": [],
   "source": [
    "# Learning rate scheduler callback\n",
    "lr_scheduler = LRScheduler(policy=\"StepLR\", step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy29DspnXU-S"
   },
   "outputs": [],
   "source": [
    "# Progressbar callback\n",
    "progressbar = ProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u4yfIEH2rPZE",
    "outputId": "af7fad5c-f72c-4a7c-f302-0edf072bf0ea"
   },
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "writer = SummaryWriter()\n",
    "#%tensorboard --logdir 'runs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually specify hyperparamers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncfnet = NeuralNet(\n",
    "    ncf,\n",
    "    module__users_emb=train.users_emb,\n",
    "    module__movies_emb=train.movies_emb,\n",
    "    module__users_ohe=train.users_ohe,\n",
    "    module__movies_ohe=train.movies_ohe,\n",
    "    module__interact=train.interact,\n",
    "    module__size_emb=30,\n",
    "    module__dropout=0.5,\n",
    "    module__linear_size=200,\n",
    "    module__y_range=train.y_range,#### Manually specify hyperparamers \n",
    "    max_epochs=30,\n",
    "    lr=0.0005,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    device=device,\n",
    "    iterator_train__batch_size=1024,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__batch_size=4096,\n",
    "    train_split=predefined_split(valid_dataset),\n",
    "    callbacks=[\n",
    "               earlystopping,\n",
    "               epoch_rmse,\n",
    "               #checkpoint,\n",
    "               lr_scheduler,\n",
    "               #TensorBoard(writer),\n",
    "               #progressbar\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ncfnet.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'module__size_emb': [30, 60, 120],\n",
    "    'module__dropout': [0.2, 0.5],\n",
    "    'module__linear_size': [100, 150, 200, 400]\n",
    "}\n",
    "gs = GridSearchCV(ncfnet,\n",
    "                  params,\n",
    "                  verbose=50,\n",
    "                  refit=False,\n",
    "                  #pre_dispatch=8,\n",
    "                  n_jobs=8,\n",
    "                  cv=2,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "X_ds = SliceDataset(train, idx=0)\n",
    "y_ds = SliceDataset(train, idx=1)\n",
    "gs.fit(X_ds, y_ds)\n",
    "\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep and Wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually specify hyperparamers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgVTuwedriTI"
   },
   "outputs": [],
   "source": [
    "deepnwidenet = NeuralNet(\n",
    "    deepnwide,\n",
    "    module__users_emb=train.users_emb,\n",
    "    module__movies_emb=train.movies_emb,\n",
    "    module__users_ohe=train.users_ohe,\n",
    "    module__movies_oh#### Manually specify hyperparamers e=train.movies_ohe,\n",
    "    module__interact=train.interact,\n",
    "    module__size_emb=30,\n",
    "    module__y_range=train.y_range,\n",
    "    module__dropout=0.2,\n",
    "    max_epochs=30,\n",
    "    lr=0.001,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    device=device,\n",
    "    iterator_train__batch_size=1024,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__batch_size=4096,\n",
    "    train_split=predefined_split(valid_dataset),\n",
    "    callbacks=[\n",
    "               earlystopping,\n",
    "               epoch_rmse,\n",
    "               #checkpoint,\n",
    "               lr_scheduler,\n",
    "               #TensorBoard(writer),\n",
    "               #progressbar\n",
    "               ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6qm5_snshUi"
   },
   "outputs": [],
   "source": [
    "#deepnwidenet.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "s4Ecaxc4euD1",
    "outputId": "2cea73fa-450e-48aa-8b0f-77e47127abda"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'module__size_emb': [30, 60, 120],\n",
    "    'module__dropout': [0.2, 0.5]\n",
    "}\n",
    "gs = GridSearchCV(deepnwidenet,\n",
    "                  params,\n",
    "                  verbose=50,\n",
    "                  refit=False,\n",
    "                  #pre_dispatch=8,\n",
    "                  n_jobs=8,\n",
    "                  cv=3,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "X_ds = SliceDataset(train, idx=0)\n",
    "y_ds = SliceDataset(train, idx=1)\n",
    "gs.fit(X_ds, y_ds)\n",
    "\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two embeddings - Basic matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually specify hyperparamers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_9-m9v_3q03"
   },
   "outputs": [],
   "source": [
    "twoembedsnet = NeuralNet(\n",
    "    twoembeds,\n",
    "    module__size_emb=128,\n",
    "    module__y_range=train.y_range,\n",
    "    max_epochs=30,\n",
    "    lr=0.001,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    device=device,\n",
    "    iterator_train__batch_size=4096,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__batch_size=4096,\n",
    "    train_split=predefined_split(valid_dataset),\n",
    "    callbacks=[earlystopping,\n",
    "               epoch_rmse,\n",
    "               #checkpoint,\n",
    "               lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9B_Z4LA6CPL"
   },
   "outputs": [],
   "source": [
    "twoembedsnet.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "G2QlpxyRTx7b",
    "outputId": "35483166-0d89-4148-c625-50c8884a9903"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'module__size_emb': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "}\n",
    "gs = GridSearchCV(twoembedsnet,\n",
    "                  params,\n",
    "                  verbose=50,\n",
    "                  refit=False,\n",
    "                  #pre_dispatch=8,\n",
    "                  n_jobs=8,\n",
    "                  cv=3,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "X_ds = SliceDataset(train, idx=0)\n",
    "y_ds = SliceDataset(train, idx=1)\n",
    "gs.fit(X_ds, y_ds)\n",
    "\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_WP8cIqZQWN"
   },
   "source": [
    "### Benchmark with scikit-surprise SVD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pjhfBV6ZRUH"
   },
   "outputs": [],
   "source": [
    "#!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsJ69IyMZTcZ"
   },
   "outputs": [],
   "source": [
    "from surprise import NormalPredictor\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate, train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 3.,  ..., 1., 4., 4.])"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[dataloaders['train'].dataset.indices][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UG7J_vXTZjcf"
   },
   "outputs": [],
   "source": [
    "user = train[dataloaders['train'].dataset.indices][0][0][:, 0].data.numpy()\n",
    "movie = train[dataloaders['train'].dataset.indices][0][2][:, 0].data.numpy()\n",
    "y = train[dataloaders['train'].dataset.indices][1].data.numpy()\n",
    "df = pd.DataFrame({'user': user, 'movie': movie, 'y': y})\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['user', 'movie', 'y']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Khqq2oRQwyZ3"
   },
   "outputs": [],
   "source": [
    "data = Dataset.load_from_df(train.ratings.loc[dataloaders['train'].dataset.indices, ['UserID', 'MovieID', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZULF4SZexU81"
   },
   "outputs": [],
   "source": [
    "a = train.ratings.loc[dataloaders['train'].dataset.indices, ['UserID', 'MovieID', 'Rating']]\n",
    "b = pd.DataFrame({'UserID': user, 'MovieID': movie, 'Rating': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "osa_K_dkckBG"
   },
   "outputs": [],
   "source": [
    "#data = Dataset.load_builtin('ml-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePIcLN-Vawop"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8917552064774448"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHY401zJgx8I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "movielens",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
